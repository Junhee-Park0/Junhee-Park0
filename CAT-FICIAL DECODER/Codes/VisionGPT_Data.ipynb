{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["aG5uadYZcpt1","iLqZehLdcvUO","7lMBEMIXeOQe"],"authorship_tag":"ABX9TyM4840DAH4S5a7iQxlrDYw1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### 필요 라이브러리"],"metadata":{"id":"aG5uadYZcpt1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZJ6UV7Ja1zq"},"outputs":[],"source":["# 데이터 처리\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import json\n","import itertools\n","import cv2\n","from torch.utils.data import DataLoader\n","\n","# ViT\n","from timm import create_model, list_models\n","from types import SimpleNamespace\n","\n","# GPT\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\n","\n","# 데이터 증강\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2 # 데이터 -> PyTorch 텐서로 변환\n","from PIL import Image # 이미지 처리\n","from pathlib import Path # 파일 경로 관리\n","from sklearn.model_selection import train_test_split\n","from torch.cuda.amp import GradScaler, autocast\n","from tqdm.auto import tqdm # 진행 상태 표시\n","import gc\n","\n","# 최종 예측\n","import subprocess\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Video"]},{"cell_type":"markdown","source":["### Fine-tuning the Model"],"metadata":{"id":"QdqR4yYZcsjO"}},{"cell_type":"markdown","source":["#### Loading the Data"],"metadata":{"id":"iLqZehLdcvUO"}},{"cell_type":"code","source":["tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.pad_token"],"metadata":{"id":"sW1usTMecxVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 데이터 불러오기\n","base_path = '/content/drive/MyDrive/DATA_최종'\n","images_path = os.path.join(base_path, 'Images')\n","labels_path = os.path.join(base_path, 'Labels')"],"metadata":{"id":"Ut4GmCqtc0Gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 영상 프레임 가져오기\n","\n","# 행동 폴더 이름 -> action_classes\n","action_classes = os.listdir(images_path)\n","print(f\"고냥이의 행동 {len(action_classes)}개 있어요 ฅ^._.^ฅ\")\n","\n","# 각 폴더로 들어가서 - 비디오 이름들 가져오고 - 그 비디오 폴더 안으로 다시 들어가서 - 비디오 프레임들 가져오기 (200개 정도만?)\n","frames = {}\n","videos = {}\n","\n","for action in action_classes:\n","    videos[action] = [] # 초기화\n","    frames[action] = [] # 초기화\n","\n","    images_path_behav = os.path.join(images_path, action) # 각 폴더 경로\n","    video_names = os.listdir(images_path_behav) # 비디오 이름들 가져오기\n","\n","    count = 0\n","    for video in video_names:\n","        if count >= 100:\n","          break\n","        frames_path = os.path.join(images_path_behav, video) # 각 영상 경로\n","        frame_names = os.listdir(frames_path) # 각 영상의 프레임 이름들\n","\n","        if not frame_names:\n","          print(f\"경고: {frames_path} 폴더가 비어 있습니다. 건너뜁니다.\")\n","          continue\n","\n","        one_video_frames = [os.path.join(frames_path, fname) for fname in frame_names]\n","        frames[action].append(one_video_frames)\n","        videos[action].append(video)\n","        count += 1"],"metadata":{"id":"sxa1tUQdc28p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 영상 텍스트 가져와서 labels에 저장\n","labels = []\n","for action in videos.keys(): # 각 행동별로\n","    for video in videos[action]: # 각 비디오별로\n","        label_path = os.path.join(labels_path, action, video) + '.json' # json 파일 경로 설정\n","        try:\n","          with open(label_path, 'r') as f:\n","              json_file = json.load(f)\n","              labels.append(\"고양이가 \" + json_file['metadata']['owner']['situation'] + \" \" + json_file['metadata']['action']) # situation, action 불러오기\n","        except Exception as e:\n","          print(f\"{label_path} 에서 오류 발생: {e}\")\n"],"metadata":{"id":"GJTRxtW6c72E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deepl"],"metadata":{"id":"5cjEoMSjc-GU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import deepl\n","\n","AUTH_KEY = \"##########\"\n","translator = deepl.Translator(AUTH_KEY)\n","\n","batch_size = 10\n","translated_labels = []\n","\n","for i in range(0, len(labels), batch_size):\n","    batch = labels[i:i+batch_size]\n","    translated_results = translator.translate_text(batch, source_lang=\"KO\", target_lang=\"EN-US\")\n","    translated_labels.extend([result.text for result in translated_results])"],"metadata":{"id":"1LePa6dDc_U2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_videos = list(itertools.chain(*videos.values()))\n","all_frames = list(itertools.chain(*frames.values()))"],"metadata":{"id":"ATaDFDPpdKm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 데이터프레임으로\n","kitties = pd.DataFrame({'video_names' : all_videos, 'videos' : all_frames, 'caption' : translated_labels})\n","kitties.head(2)"],"metadata":{"id":"Zwa681ApdLuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 잘 로드되었는지 확인\n","image_path = kitties['videos'][5][13]\n","image = cv2.imread(image_path)\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)"],"metadata":{"id":"yPt63kQqdNTr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset, DataLoader"],"metadata":{"id":"7lMBEMIXeOQe"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, df, tokenizer, transform):\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        sample = self.df.iloc[idx, : ]\n","        frame_paths = sample['videos']\n","        caption = sample['caption']\n","\n","        # 이미지 불러오기\n","        frames = []\n","        for frame_path in frame_paths:\n","            image = cv2.imread(frame_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = np.array(image, dtype = np.uint8)\n","            image = self.transform(image = image)[\"image\"] # 바뀐 애들 중에서 image만 떼어오기\n","            frames.append(image)\n","\n","        frames_tensor = torch.stack([torch.as_tensor(frame, dtype = torch.float32).clone().detach() for frame in frames])\n","\n","        # 텍스트 토큰화\n","        caption = f\"{caption}<|endoftext|>\"\n","        input_ids = self.tokenizer(\n","            caption,\n","            truncation=True,\n","            return_tensors = \"pt\")['input_ids'].squeeze(0) # squeeze by gpt\n","        labels = input_ids.clone()\n","        labels[ :-1] = input_ids[1: ]\n","        return frames_tensor, input_ids, labels"],"metadata":{"id":"8ILGWwv4dQIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_kitties, val_kitties = train_test_split(kitties, test_size=0.1)"],"metadata":{"id":"Nt8roYZQdWdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_kitties = Dataset(train_kitties, tokenizer, transform)\n","val_kitties = Dataset(val_kitties, tokenizer, transform)"],"metadata":{"id":"LOuz2PIUdXFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 영상들의 프레임 개수 리스트 - 얼마로 패딩할지 정하기 위함 (평균 프레임 개수로 정했습니당)\n","frames_num = []\n","for i in range(len(kitties)):\n","    frames_num.append(len(kitties.loc[i, 'videos']))\n","\n","video_padding = np.mean(frames_num)"],"metadata":{"id":"zoq1twnldXvm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    frames, input_ids, labels = zip(*batch)\n","\n","    # 이미지 패딩\n","    padding_size = int(video_padding)\n","    padded_frames = []\n","\n","    for video in frames:\n","        current_length = video.shape[0]\n","\n","        if current_length > padding_size: # 패딩보다 길다면\n","            video = video[ :padding_size] # 잘라내고\n","        else:\n","            pad_size = padding_size - current_length # 짧으면 부족한 만큼 채우기\n","            pad_tensor = torch.zeros((pad_size, *video.shape[1: ]))\n","            video = torch.cat([video, pad_tensor], dim = 0)\n","\n","        padded_frames.append(video)\n","\n","    padded_frames_together = torch.stack(padded_frames)\n","\n","    # 텍스트 패딩\n","    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first = True)\n","    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first = True, padding_value = -100) # 나중에 loss 계산할 때 무시하라고 -100으로 처리\n","\n","    return padded_frames_together, input_ids, labels"],"metadata":{"id":"BGXKvAsWdYgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### DataLoader 설정\n","train_dataloader = DataLoader(\n","    train_kitties,\n","    batch_size = 4,\n","    shuffle = True,\n","    num_workers = 1,\n","    collate_fn = collate_fn)\n","\n","val_dataloader = DataLoader(\n","    val_kitties,\n","    batch_size = 4,\n","    shuffle = False,\n","    num_workers = 1,\n","    collate_fn = collate_fn)"],"metadata":{"id":"_pCR3OcvdZgI"},"execution_count":null,"outputs":[]}]}