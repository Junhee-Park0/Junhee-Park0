{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1kEk4S4UpF5f1JMV7VbMMl_O4cw0UQLca","authorship_tag":"ABX9TyMmIde5Sr9otPJVxot4MRGs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"79de84cd4e2d4626811cd80eee1be81a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a99fa4251fb846ed9f655e3a3e92150e","IPY_MODEL_c40549fa2f3a48a6a43a9c0e4c3ac234","IPY_MODEL_42a4491ade074a898e334207c0cfaf1d"],"layout":"IPY_MODEL_87aa1ca0483a43858852702d32a0bf97"}},"a99fa4251fb846ed9f655e3a3e92150e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb821dcd1e7248b5941e7497683f4597","placeholder":"​","style":"IPY_MODEL_aacaa3f179274e3f9a585fae5f95d005","value":"tokenizer_config.json: 100%"}},"c40549fa2f3a48a6a43a9c0e4c3ac234":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_195ed021b5a74f1b9a50a707c6135595","max":1948,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e538cf8e95b344e98786344002dd9657","value":1948}},"42a4491ade074a898e334207c0cfaf1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de275defb9de4930a9a35ceace057d32","placeholder":"​","style":"IPY_MODEL_7bb9b9f93aae40fdbb82def9266c71ff","value":" 1.95k/1.95k [00:00&lt;00:00, 109kB/s]"}},"87aa1ca0483a43858852702d32a0bf97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb821dcd1e7248b5941e7497683f4597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aacaa3f179274e3f9a585fae5f95d005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"195ed021b5a74f1b9a50a707c6135595":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e538cf8e95b344e98786344002dd9657":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de275defb9de4930a9a35ceace057d32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bb9b9f93aae40fdbb82def9266c71ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"465bf450af544ba89c5eba8d13899818":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d08f4e1873841cab05690086c5d8503","IPY_MODEL_06600ed8bb3a46aaac74eddd84347b62","IPY_MODEL_cd4c739f78f64420a3a6bc3e59e6abd3"],"layout":"IPY_MODEL_9111c6f18d634aee8f62fb671d154c8a"}},"3d08f4e1873841cab05690086c5d8503":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00a5b6bce154afa91c228742e070f01","placeholder":"​","style":"IPY_MODEL_d3d34c8782d44bc7be18b60ca3a3fb8a","value":"tokenizer.json: 100%"}},"06600ed8bb3a46aaac74eddd84347b62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7a059c06acb4127aeb33bea7853b5e9","max":2919731,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4b7aaba81c0477fa73dc2591913ae8b","value":2919731}},"cd4c739f78f64420a3a6bc3e59e6abd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ee510eec54e4024849bdb7b62603e84","placeholder":"​","style":"IPY_MODEL_06ab32b1114b4cd8b0c2eef31a29b162","value":" 2.92M/2.92M [00:00&lt;00:00, 36.5MB/s]"}},"9111c6f18d634aee8f62fb671d154c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a00a5b6bce154afa91c228742e070f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3d34c8782d44bc7be18b60ca3a3fb8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7a059c06acb4127aeb33bea7853b5e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b7aaba81c0477fa73dc2591913ae8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ee510eec54e4024849bdb7b62603e84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06ab32b1114b4cd8b0c2eef31a29b162":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66a6de3f325d4c1c923f90206d5f47a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf7dd0d8660c4b2aa8262cbceed94e07","IPY_MODEL_c05d1dd160f04962b2d9fdc5eef6e08d","IPY_MODEL_6dc978ef38364321802f8668684a1a5d"],"layout":"IPY_MODEL_2625d51219954c5e845eb10eac2be52c"}},"bf7dd0d8660c4b2aa8262cbceed94e07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acf93503ee8c4ea99790452e2afb80be","placeholder":"​","style":"IPY_MODEL_f78cbea944414d06a14644a1ff5980f6","value":"special_tokens_map.json: 100%"}},"c05d1dd160f04962b2d9fdc5eef6e08d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16854e5804704d009b5bd22d919aeb9d","max":1786,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eb9baf232e14b91b5194e7575b4db5e","value":1786}},"6dc978ef38364321802f8668684a1a5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_175044d109fa4ba98756e1e9b418eee5","placeholder":"​","style":"IPY_MODEL_54afa8baf27d4ad59a6d876ef8006cdd","value":" 1.79k/1.79k [00:00&lt;00:00, 86.6kB/s]"}},"2625d51219954c5e845eb10eac2be52c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acf93503ee8c4ea99790452e2afb80be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f78cbea944414d06a14644a1ff5980f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16854e5804704d009b5bd22d919aeb9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eb9baf232e14b91b5194e7575b4db5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"175044d109fa4ba98756e1e9b418eee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54afa8baf27d4ad59a6d876ef8006cdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5txSzdULST9J","executionInfo":{"status":"ok","timestamp":1733013723614,"user_tz":-540,"elapsed":274,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"f788a207-d907-400a-9156-e025a3faa5a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import re\n","\n","import torch\n","import torch.nn as nn\n","import transformers\n","from collections import Counter\n","from transformers import T5TokenizerFast, T5ForConditionalGeneration, AdamW\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from transformers import TrainerCallback\n","import matplotlib.pyplot as plt\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","source":["### Data 로드"],"metadata":{"id":"YbYOLMFeSW8a"}},{"cell_type":"code","source":["def open_json(url):\n","  with open(url) as f:\n","    data = json.load(f)\n","  return data"],"metadata":{"id":"FF8rg4OCSWg6","executionInfo":{"status":"ok","timestamp":1733013730543,"user_tz":-540,"elapsed":318,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 초등\n","train1 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Training/01.원천데이터/TS_01. 학교급_01. 초등/상담기록_데이터_초등학교.json')\n","# 중등\n","train2 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Training/01.원천데이터/TS_01. 학교급_02. 중등/상담기록_데이터_중학교.json')\n","# 고등\n","train3 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Training/01.원천데이터/TS_01. 학교급_03. 고등/상담기록_데이터_고등학교.json')"],"metadata":{"id":"WiezOXetSdTZ","executionInfo":{"status":"ok","timestamp":1733013738736,"user_tz":-540,"elapsed":6582,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 초등\n","test1 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Validation/01.원천데이터/VS_01. 학교급_01. 초등/상담기록_데이터_초등학교.json')\n","# 중등\n","test2 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Validation/01.원천데이터/VS_01. 학교급_02. 중등/상담기록_데이터_중학교.json')\n","# 고등\n","test3 = open_json('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/DATA/01-1.정식개방데이터/Validation/01.원천데이터/VS_01. 학교급_03. 고등/상담기록_데이터_고등학교.json')"],"metadata":{"id":"wHu1LTy1SqMO","executionInfo":{"status":"ok","timestamp":1733013741606,"user_tz":-540,"elapsed":1788,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 함수 정의"],"metadata":{"id":"ECFEcxnZS-ik"}},{"cell_type":"markdown","source":["#### 대화 추출"],"metadata":{"id":"N0nDfOdnS3_M"}},{"cell_type":"code","source":["def get_conversation(data):\n","  conversation = []\n","\n","  for i in range(len(data)): # 각 학생별로\n","    student = data[str(i)]['conversation'] # 각 학생의 대화\n","    # student['conversation'] # 대화 내용\n","    # print(len(student))\n","\n","    for j in range(len(student)):\n","      talk = student[j]['utterances'] # 각 학생의 진짜 대화\n","      utter = []\n","      first_speech = True\n","\n","      for k in range(len(talk)):\n","\n","        if talk[k]['speaker_idx'].startswith(\"S\"): # 학생이 하는 말이면\n","          if (\n","              any(word in talk[k]['utterance'] for word in [\"아니오\",\"아니요\",\"안녕\",\"감사\"]) # 얘네가 있으면\n","              or talk[k]['utterance'] in ['네!', '네 [이모티콘]', '네', '네~', '네~!', '네.', '넵', '넵!', '넵~', '넵 [이모티콘]', '네 하하', '하하하', '하하','넵 하하']):  # 네 라면\n","            continue\n","\n","        if talk[k]['speaker_idx'].startswith(\"T\") and talk[k]['utterance'] == '파일':\n","          continue\n","\n","        if len(talk[k]['utterance']) < 5: # 10글자 이하라면\n","          continue\n","\n","        if first_speech: # 각 학생의 첫 번째 대화\n","          speaker = talk[k]['speaker_idx']\n","          utter = [talk[k]['utterance']]\n","          first_speech = False\n","\n","        else: # 그 다음 대화\n","          if talk[k]['speaker_idx'] == speaker: # 같은 사람이면\n","            utter.append(talk[k]['utterance']) # 이번 발화를 저번 발화에 이어 붙이고\n","\n","          else: # 다른 사람이 되면\n","            conversation.append({\n","                'speaker' : speaker,\n","                'utterance' : ' '.join(utter)\n","            })\n","            utter = [talk[k]['utterance']]\n","\n","          speaker = talk[k]['speaker_idx'] # speaker 갱신\n","\n","      if not first_speech and utter:\n","        conversation.append({\n","            'speaker' : speaker,\n","            'utterance' : ' '.join(utter)\n","        })\n","\n","  return conversation"],"metadata":{"id":"iPABtR1MTBXg","executionInfo":{"status":"ok","timestamp":1733013744366,"user_tz":-540,"elapsed":326,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#### 기호, [이모티콘] 처리"],"metadata":{"id":"rBtmuj1JS-PO"}},{"cell_type":"code","source":["def remove_punc(data):\n","  for sent in data:\n","    # '[이모티콘]' -> ~\n","    sent['utterance'] = sent['utterance'].replace('[이모티콘]','')\n","    # 링크 제거\n","    sent['utterance'] = re.sub(r'https://\\S+', '', sent['utterance'])\n","    sent['utterance'] = re.sub(r'http://\\S+', '', sent['utterance'])\n","    # , . ? ! 는 그대로 살려 (앞뒤 공백만 추가) -> 공백이 너무 많아서 일단 이거 제거\n","    # sent['utterance'] = re.sub(r'([,.~!?])', r' \\1 ', sent['utterance'])\n","    # 한글, 영어, 숫자, 앞서 살리기로 한 기호가 아니면 제거\n","    sent['utterance'] = re.sub(r'[^ㄱ-ㅎ가-힣,.~!?\\d\\w ]', '', sent['utterance'])\n","\n","  return data"],"metadata":{"id":"zzNHuhnVTUTn","executionInfo":{"status":"ok","timestamp":1733013746360,"user_tz":-540,"elapsed":257,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#### 띄어쓰기 한 번으로"],"metadata":{"id":"eTlNDRn-J97D"}},{"cell_type":"code","source":["def one_space(data):\n","  for sent in data:\n","    sent['utterance'] = re.sub(r'\\s+',' ', sent['utterance'])\n","  return data"],"metadata":{"id":"26fa9nZ5KAv2","executionInfo":{"status":"ok","timestamp":1733013748465,"user_tz":-540,"elapsed":392,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#### 질문 / 답변 페어링"],"metadata":{"id":"UBzWkUsvTdkr"}},{"cell_type":"code","source":["def make_pairs(data):\n","  # pairs = pd.DataFrame()\n","  questions = []\n","  answers = []\n","\n","  for i in range(len(data)-1): # 각 줄 별로\n","\n","    num = 2*i\n","\n","    if num+1 == len(data):\n","      break\n","\n","    question = data[num]['utterance']\n","    answer = data[num+1]['utterance']\n","\n","    questions.append(question)\n","    answers.append(answer)\n","\n","\n","  # pairs['Question'] = questions\n","  # pairs['Answer'] = answers\n","\n","  return questions, answers"],"metadata":{"id":"zlMUhRB9TizR","executionInfo":{"status":"ok","timestamp":1733014085057,"user_tz":-540,"elapsed":344,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["### 함수 적용 - 전처리 완료"],"metadata":{"id":"d5J2FyjOTk-e"}},{"cell_type":"code","source":["# 대화 추출\n","train_conv1 = get_conversation(train1)\n","train_conv2 = get_conversation(train2)\n","train_conv3 = get_conversation(train3)\n","# ------\n","test_conv1 = get_conversation(test1)\n","test_conv2 = get_conversation(test2)\n","test_conv3 = get_conversation(test3)"],"metadata":{"id":"WEEQHp8BTDnE","executionInfo":{"status":"ok","timestamp":1733013751688,"user_tz":-540,"elapsed":1379,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 통합\n","train_conv = train_conv1 + train_conv2 + train_conv3\n","test_conv = test_conv1 + test_conv2 + test_conv3"],"metadata":{"id":"tY3DfCcCTN9T","executionInfo":{"status":"ok","timestamp":1733013752819,"user_tz":-540,"elapsed":250,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 기호 제거\n","train_wo_punc = remove_punc(train_conv)\n","test_wo_punc = remove_punc(test_conv)"],"metadata":{"id":"yJfGYEuxTWZa","executionInfo":{"status":"ok","timestamp":1733013755806,"user_tz":-540,"elapsed":1456,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 띄어쓰기 제거\n","train_wo_space = one_space(train_wo_punc)\n","test_wo_space = one_space(test_wo_punc)"],"metadata":{"id":"NjWRKOFuKNfQ","executionInfo":{"status":"ok","timestamp":1733013759213,"user_tz":-540,"elapsed":2662,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 질문-답변 페어링\n","train_question, train_answer = make_pairs(train_wo_space)\n","test_question, test_answer = make_pairs(test_wo_space)"],"metadata":{"id":"c5uVxdrfTu6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_question[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qSdu_Y7JapU","executionInfo":{"status":"ok","timestamp":1732973182156,"user_tz":-540,"elapsed":6,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"5edcec90-783f-49b3-f51b-37d520df4877"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['어서 오세요 방학인데 휴가는 다녀왔어요? 어디로 다녀왔나요? 네~ 조부모님 댁인가요?',\n"," '여행이요 하하',\n"," '학생은 형제자매가 어떻게 돼요?',\n"," '여동생 한 명이에요!',\n"," '동생이 몇 살인가요?',\n"," '9살이에요',\n"," '동생이 있어서 좋은 점은 뭘까요?',\n"," '심심하지 않다는 점?',\n"," '네~ 그럼 불편한 점도 있나요?',\n"," '많이 시끄러워요 장난도 심해요']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["train_answer[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDipa-Rae9vT","executionInfo":{"status":"ok","timestamp":1732973182156,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"5f7a3651-4f4b-4dbc-a87e-45bec65d3b21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['여행이요 하하',\n"," '학생은 형제자매가 어떻게 돼요?',\n"," '여동생 한 명이에요!',\n"," '동생이 몇 살인가요?',\n"," '9살이에요',\n"," '동생이 있어서 좋은 점은 뭘까요?',\n"," '심심하지 않다는 점?',\n"," '네~ 그럼 불편한 점도 있나요?',\n"," '많이 시끄러워요 장난도 심해요',\n"," '어떤 장난을 해요?']"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["#### 문장 길이 확인"],"metadata":{"id":"Cpf7Y7hySVfM"}},{"cell_type":"code","source":["train_question_len = [len(sent) for sent in train_question]\n","train_answer_len = [len(sent) for sent in train_answer]"],"metadata":{"id":"ru8clNaoSXPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pd.Series(train_question_len).describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xk627vL-TCd-","executionInfo":{"status":"ok","timestamp":1732973182156,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"103c6ff1-aa06-4533-ac24-767111a3b649"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["count    302072.000000\n","mean         49.247640\n","std          70.344794\n","min           0.000000\n","25%          16.000000\n","50%          30.000000\n","75%          57.000000\n","max        5100.000000\n","dtype: float64\n"]}]},{"cell_type":"code","source":["print(pd.Series(train_answer_len).describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUj5PBDxTWXm","executionInfo":{"status":"ok","timestamp":1732973183016,"user_tz":-540,"elapsed":862,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"bbaf3b99-5540-408e-9d91-5ee0a17c17dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["count    302072.000000\n","mean         49.247818\n","std          70.344856\n","min           0.000000\n","25%          16.000000\n","50%          30.000000\n","75%          57.000000\n","max        5100.000000\n","dtype: float64\n"]}]},{"cell_type":"markdown","source":["### Tokenize & Encoding"],"metadata":{"id":"Prplh-O4Tub7"}},{"cell_type":"code","source":["## paust/pko-chat-t5-large <- 실패..\n","tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-small')\n","max_length = 70"],"metadata":{"id":"uxzwdGgdT5U2","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 토큰 개수 확인"],"metadata":{"id":"iZT3KteuJJu3"}},{"cell_type":"code","source":["for sent in train_wo_space[:10]:\n","  print(sent['utterance'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-w1o0G19PzyG","executionInfo":{"status":"ok","timestamp":1732939606655,"user_tz":-540,"elapsed":943,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"9c173659-da33-4268-9d23-c767838ad8fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["어서 오세요 방학인데 휴가는 다녀왔어요? 어디로 다녀왔나요? 네~ 조부모님 댁인가요?\n","여행이요 하하\n","학생은 형제자매가 어떻게 돼요?\n","여동생 한 명이에요!\n","동생이 몇 살인가요?\n","9살이에요\n","동생이 있어서 좋은 점은 뭘까요?\n","심심하지 않다는 점?\n","네~ 그럼 불편한 점도 있나요?\n","많이 시끄러워요 장난도 심해요\n"]}]},{"cell_type":"code","source":["check_tokens = []\n","for sent in train_wo_space: # 페어링 안 된 상태에서\n","  token_ids = tokenizer.encode(sent['utterance']) # 토큰화된 상태의 id\n","  tokens = [tokenizer.decode([id]) for id in token_ids] # id -> 토큰으로\n","  check_tokens.append(tokens)"],"metadata":{"id":"nchZIXxEfhmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flattened = [token for tokens in check_tokens for token in tokens]"],"metadata":{"id":"XC9wsNxhm0wP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counts = Counter(flattened)"],"metadata":{"id":"0T51ILZBlVRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counts.most_common(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFPPNmXahSDl","executionInfo":{"status":"ok","timestamp":1732939873097,"user_tz":-540,"elapsed":515,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"9ecaf2b3-c002-4b51-f1c9-8a59c157283c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(' ', 3463372),\n"," ('요', 322884),\n"," ('</s>', 302073),\n"," ('.', 151358),\n"," ('이', 147261),\n"," ('?', 139914),\n"," ('을', 123349),\n"," ('에', 102827),\n"," (',', 98579),\n"," ('것', 86706)]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["#### ChatBotDataset Class 정의"],"metadata":{"id":"vqXQDoGbSOn_"}},{"cell_type":"code","source":["class ChatBotDataset(Dataset):\n","  def __init__(self, questions, answers, tokenizer, max_length):\n","    self.questions = questions\n","    self.answers = answers\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","\n","  def __len__(self):\n","    return len(self.questions)\n","\n","  def __getitem__(self, idx): # 한 문장씩 받아와서\n","    question = self.questions[idx] # 문자열인지 확인\n","    answer = self.answers[idx]\n","\n","    inputs = self.tokenizer(question,\n","                            max_length=self.max_length,\n","                            padding='max_length',\n","                            truncation=True,\n","                            return_tensors='pt')\n","\n","    labels = self.tokenizer(answer,\n","                            max_length=self.max_length,\n","                            padding='max_length',\n","                            truncation=True,\n","                            return_tensors = 'pt')\n","\n","    return{\n","        'input_ids' : inputs.input_ids[0],\n","        'labels' : labels.input_ids[0]\n","    }"],"metadata":{"id":"-AQSotOHcPl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = ChatBotDataset(train_question, train_answer, tokenizer, max_length)\n","dev_data = ChatBotDataset(test_question, test_answer, tokenizer, max_length)"],"metadata":{"id":"cgJoHH0XePEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모델 불러오기"],"metadata":{"id":"VsOtCfsvnKnH"}},{"cell_type":"code","source":["# model 정의\n","## paust/pko-chat-t5-large <- 실패..\n","model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-small')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"asR5GaC7Z6cw","executionInfo":{"status":"ok","timestamp":1732980442025,"user_tz":-540,"elapsed":1555,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"f76ea826-8ee5-4a2b-e7c5-2e640798aea9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(50358, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50358, bias=False)\n",")"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["### 학습 진행"],"metadata":{"id":"kptM2Ra_nM1_"}},{"cell_type":"code","source":["# optimizer, loss function은 모델에 내장된 거 사용\n","training_arguments = TrainingArguments(\n","    # fp16 = True, # 메모리 줄이기 위함 - 성능이 약간은 떨어질 수 있다고 하지만 일단은... 시도해보자\n","    output_dir = '/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model',\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    learning_rate = 2e-3, # 기존 2e-5\n","    per_device_train_batch_size = 64, # 기존 64\n","    per_device_eval_batch_size = 64, # 기존 64\n","    gradient_accumulation_steps = 2, # 중간에  GPU가 부족하면 batch size 조정\n","    num_train_epochs = 5,\n","    weight_decay = 0.01,\n","    logging_dir = './logs',\n","    logging_steps = 500,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'eval_loss',\n","    greater_is_better = False,\n","    save_total_limit = 2, # 저장할 체크포인트 개수 (최신 2개만)\n","    report_to = 'none' # W&B 비활성화,,\n",")\n","\n","early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience = 3,\n","    early_stopping_threshold = 0.001\n",")\n","\n","class SaveModelCallback(TrainerCallback): # epoch 완료시 저장\n","\n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        save_path = f\"/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-{state.epoch}\"\n","        print(f\"Epoch {state.epoch} 완료. 모델 저장 중: {save_path}\")\n","        kwargs['model'].save_pretrained(save_path)\n","\n","trainer = Trainer(\n","    model = model,\n","    args = training_arguments,\n","    callbacks = [early_stopping_callback, SaveModelCallback()],\n","    train_dataset = train_data,\n","    eval_dataset = dev_data\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFc1-LAXeJPT","executionInfo":{"status":"ok","timestamp":1732983123396,"user_tz":-540,"elapsed":481,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"9af1cf42-e2ab-4746-e283-11f41067395c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#### 학습 첫 실행"],"metadata":{"id":"8wIHGx4h2t8z"}},{"cell_type":"code","source":["trainer.train(resume_from_checkpoint = False)\n","trainer.evaluate()\n","\n","model.save_pretrained('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/End/')\n","tokenizer.save_pretrained('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/End/')\n","print(\" 모델 저장 완료! :D \")"],"metadata":{"id":"tEECCzLd2q5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 학습 이어서 하기"],"metadata":{"id":"9Ju1q1Ab22VU"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-1.0\" ## 어디서 멈췄는지 알아야\n","model = T5ForConditionalGeneration.from_pretrained(model_path)\n","tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-small')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"VtHlq8kfqBr_","executionInfo":{"status":"ok","timestamp":1732985194994,"user_tz":-540,"elapsed":4447,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"17a57eca-d368-467b-91d4-a550ff484753"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(50358, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50358, bias=False)\n",")"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["trainer = Trainer(\n","    model = model,\n","    args = training_arguments,\n","    callbacks = [early_stopping_callback, SaveModelCallback()],\n","    train_dataset = train_data,\n","    eval_dataset = dev_data\n",")"],"metadata":{"id":"Wa2wL_0TrEtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","  trainer.train(resume_from_checkpoint = True)\n","  trainer.evaluate()\n","except Exception as e:\n","  print(f\"오류 발생! {e}\")\n","\n","model.save_pretrained('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/')\n","tokenizer.save_pretrained('/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/')\n","\n","print(\" 모델 저장 완료! :D \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"Op3Aom30g1dC","outputId":"d089ffac-2002-45c2-de96-24d3d5c560ea","executionInfo":{"status":"ok","timestamp":1732993286642,"user_tz":-540,"elapsed":8086337,"user":{"displayName":"박준희","userId":"01167826914931997240"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_rng_state = torch.load(rng_file)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11800' max='11800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11800/11800 2:12:53, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>2</td>\n","      <td>0.809900</td>\n","      <td>0.814245</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.772500</td>\n","      <td>0.801747</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.724700</td>\n","      <td>0.799309</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.683500</td>\n","      <td>0.803274</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 2.0 완료. 모델 저장 중: /content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-2.0\n","Epoch 3.0 완료. 모델 저장 중: /content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-3.0\n","Epoch 4.0 완료. 모델 저장 중: /content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-4.0\n","Epoch 5.0 완료. 모델 저장 중: /content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-5.0\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='602' max='602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [602/602 01:38]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" 모델 저장 완료! :D \n"]}]},{"cell_type":"markdown","source":["### 응답 생성하기"],"metadata":{"id":"GpBsfVbFn51-"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/2. KOREA UNIV./2024 Fall/청소년 데이터 공모전/Model/epoch-4.0\" ## 어디서 멈췄는지 알아야\n","model = T5ForConditionalGeneration.from_pretrained(model_path)\n","tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-small')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["79de84cd4e2d4626811cd80eee1be81a","a99fa4251fb846ed9f655e3a3e92150e","c40549fa2f3a48a6a43a9c0e4c3ac234","42a4491ade074a898e334207c0cfaf1d","87aa1ca0483a43858852702d32a0bf97","bb821dcd1e7248b5941e7497683f4597","aacaa3f179274e3f9a585fae5f95d005","195ed021b5a74f1b9a50a707c6135595","e538cf8e95b344e98786344002dd9657","de275defb9de4930a9a35ceace057d32","7bb9b9f93aae40fdbb82def9266c71ff","465bf450af544ba89c5eba8d13899818","3d08f4e1873841cab05690086c5d8503","06600ed8bb3a46aaac74eddd84347b62","cd4c739f78f64420a3a6bc3e59e6abd3","9111c6f18d634aee8f62fb671d154c8a","a00a5b6bce154afa91c228742e070f01","d3d34c8782d44bc7be18b60ca3a3fb8a","a7a059c06acb4127aeb33bea7853b5e9","f4b7aaba81c0477fa73dc2591913ae8b","4ee510eec54e4024849bdb7b62603e84","06ab32b1114b4cd8b0c2eef31a29b162","66a6de3f325d4c1c923f90206d5f47a9","bf7dd0d8660c4b2aa8262cbceed94e07","c05d1dd160f04962b2d9fdc5eef6e08d","6dc978ef38364321802f8668684a1a5d","2625d51219954c5e845eb10eac2be52c","acf93503ee8c4ea99790452e2afb80be","f78cbea944414d06a14644a1ff5980f6","16854e5804704d009b5bd22d919aeb9d","1eb9baf232e14b91b5194e7575b4db5e","175044d109fa4ba98756e1e9b418eee5","54afa8baf27d4ad59a6d876ef8006cdd"]},"id":"mIgj0y-1n7Jj","executionInfo":{"status":"ok","timestamp":1733013208625,"user_tz":-540,"elapsed":9743,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"8941e4de-0e00-43b0-a080-2f946220992d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79de84cd4e2d4626811cd80eee1be81a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"465bf450af544ba89c5eba8d13899818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a6de3f325d4c1c923f90206d5f47a9"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(50358, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(50358, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50358, bias=False)\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 이전 3개 대화 참고하게끔\n","\n","conversation_history = []\n","\n","while (1):\n","  user_input = input(\"사용자: \")\n","\n","  conversation_history.append(f\"사용자 : {user_input}\")\n","  if len(conversation_history) > 3: # 3개보다 많이 입력되면\n","    conversation_history.pop(0) # 가장 오래된 대화 제거\n","\n","  if user_input == '그만할래요':\n","    break\n","\n","  conversation_text = \"\\n\".join(conversation_history)\n","\n","  # 프롬프트 추가\n","  prompt = f\"너는 사용자의 고민과 이야기를 들어주는 진로 추천 전문 상담사야. 사용자와 대화를 주고받으며 사용자에게 가장 어울리는 진로를 추천하거나 조언을 해 줘. \\n\\n{conversation_text}\\n\\n답:\\n\"\n","  input_ids = tokenizer(prompt, return_tensors = 'pt').input_ids.to(device)\n","\n","  # 모델에 입력\n","  logits = model.generate(\n","      input_ids,\n","      max_length = 150,\n","      temperature = 0.9,\n","      no_repeat_ngram_size = 6,\n","      do_sample = True,\n","      num_return_sequences = 1\n","  )\n","\n","  # 모델 응답 추출\n","  model_response = tokenizer.batch_decode(logits, skip_special_tokens = True)[0]\n","\n","  # 답변에 응답 추가\n","  conversation_history.append(f\"답: {model_response}\")\n","  if len(conversation_history) > 3:\n","    conversation_history.pop(0)\n","\n","  print(\"답: \",model_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"id":"6PsA8epclsne","executionInfo":{"status":"error","timestamp":1733013595768,"user_tz":-540,"elapsed":110472,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"84de7f50-46ff-4a8e-8975-bf81bb0ec21d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["사용자: 뭐를 하고 싶은지 모르겠어요ㅠㅜ 도와주세요!\n","답:  네. 좋아요, 답변 부탁드릴게요.\n","사용자: MBTI 검사 결과 ENTP가 나왔는데, 저한테 어울리는 직업은 뭘까요?\n","답:  INFP가 나온 것 같아요. 제 기억으로는 앱 개발자 이런 거였던 거 같아요.\n","사용자: 저는 ENTP인걸요..\n","답:  아 그렇군요!\n","사용자: 저는 뭐를 하면 좋을까요? ENTP에게 어울리는 직업!\n","답:  그럼 과학 고등학교는 어디로 가고 싶은가요?\n","사용자: 과학 고등학교를 가면 무엇을 할 수 있죠?\n","답:  저는 대학교 때 공과대학교를 전공해서 로봇 공학 쪽으로 가고 싶어요\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-bb0eaa281089>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"사용자: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mconversation_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"사용자 : {user_input}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}
